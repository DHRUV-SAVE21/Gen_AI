{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c6d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from tavily import TavilyClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb258a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "TAVILY_KEY = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42de362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", api_key=GEMINI_KEY, temperature=0.2)\n",
    "tavily_client = TavilyClient(api_key=TAVILY_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb0892",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question(TypedDict):\n",
    "    id: str\n",
    "    type: str  \n",
    "    title: str\n",
    "    prompt: str\n",
    "    difficulty: str\n",
    "    time_limit_seconds: Optional[int]\n",
    "\n",
    "class AnswerRecord(TypedDict):\n",
    "    qid: str\n",
    "    answer_text: str\n",
    "    score: float\n",
    "    strengths: List[str]\n",
    "    weaknesses: List[str]\n",
    "    suggested_resources: List[Dict[str, str]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff6b2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_text(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Try to extract the first JSON object from LLM output robustly.\"\"\"\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "    if start == -1 or end == -1:\n",
    "        raise ValueError(\"No JSON object found in LLM response.\")\n",
    "    sub = text[start:end+1]\n",
    "    return json.loads(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59c1a023",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tavily_search(query: str, max_results: int = 5) -> List[Dict[str, str]]:\n",
    "    \"\"\"Return list of {title, url} from Tavily. Returns empty list if Tavily not configured.\"\"\"\n",
    "    if tavily_client is None:\n",
    "        return []\n",
    "    res = tavily_client.search(query=query, max_results=max_results)\n",
    "    items = res.get(\"results\", []) if isinstance(res, dict) else []\n",
    "    return [{\"title\": r.get(\"title\", \"\"), \"url\": r.get(\"url\", \"\")} for r in items[:max_results]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33ca5261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(role: str, level: str, user_skills: List[str], n_questions: int = 6) -> List[Question]:\n",
    "    \"\"\"\n",
    "    Ask Gemini to generate N tailored questions for role+level.\n",
    "    Returns a list of question dicts (Question TypedDict).\n",
    "    \"\"\"\n",
    "    skills_text = \", \".join(user_skills)\n",
    "    prompt = f\"\"\"\n",
    "You are an interview-question generator. Produce a JSON object with key \"questions\"\n",
    "which is a list of {n_questions} question objects tailored for role \"{role}\" and level \"{level}\".\n",
    "Each question object must have keys:\n",
    "  - id (string)\n",
    "  - type (behavioral|technical|coding|system-design)\n",
    "  - title\n",
    "  - prompt (what the interviewer will ask)\n",
    "  - difficulty (easy|medium|hard)\n",
    "  - time_limit_seconds (integer or null)\n",
    "\n",
    "Make technical questions focus on the user's weaker skills if provided: {skills_text}.\n",
    "Return ONLY a single JSON object. No extra commentary.\n",
    "\"\"\"\n",
    "    resp = gemini_llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    out = resp.content\n",
    "    try:\n",
    "        data = extract_json_from_text(out)\n",
    "        questions = data.get(\"questions\", [])\n",
    "        return questions\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to parse questions from LLM output: {e}\\nRaw output:\\n{out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fd88a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_suggested_resources(skills: List[str]) -> List[Dict[str, str]]:\n",
    "    \"\"\"For each skill, fetch top Tavily resources and flatten into a list (title, url).\"\"\"\n",
    "    resources = []\n",
    "    for s in skills:\n",
    "        hits = tavily_search(f\"best free resources to learn {s}\", max_results=3)\n",
    "        if hits:\n",
    "            for h in hits:\n",
    "                resources.append({\"title\": f\"{s}: {h['title']}\", \"url\": h[\"url\"]})\n",
    "        else:\n",
    "            # fallback placeholder\n",
    "            resources.append({\"title\": f\"Search resources for {s}\", \"url\": f\"https://www.google.com/search?q=learn+{s.replace(' ', '+')}\"})\n",
    "    return resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd90368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_text_mock_interview(role: str, level: str, user_skills: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    CLI flow: generate questions, present them, collect typed answers, grade with Gemini, return a session summary.\n",
    "    \"\"\"\n",
    "    print(f\"Preparing {role} ({level}) mock interview with skills: {user_skills}...\\n\")\n",
    "    questions = generate_questions(role, level, user_skills, n_questions=6)\n",
    "    answers: List[AnswerRecord] = []\n",
    "\n",
    "    for i, q in enumerate(questions, start=1):\n",
    "        print(f\"Q{i}. ({q.get('type')}) {q.get('title')}\")\n",
    "        print(q.get(\"prompt\"))\n",
    "        user_ans = input(\"\\nYour answer (type + Enter):\\n\")\n",
    "        grade = grade_answer_with_gemini(q, user_ans)\n",
    "        suggested_skills = grade.get(\"suggested_skills_to_improve\", [])\n",
    "        suggested_resources = create_suggested_resources(suggested_skills)\n",
    "        record: AnswerRecord = {\n",
    "            \"qid\": q.get(\"id\"),\n",
    "            \"answer_text\": user_ans,\n",
    "            \"score\": float(grade.get(\"score\", 0.0)),\n",
    "            \"strengths\": grade.get(\"strengths\", []),\n",
    "            \"weaknesses\": grade.get(\"weaknesses\", []),\n",
    "            \"suggested_resources\": suggested_resources,\n",
    "        }\n",
    "        answers.append(record)\n",
    "        print(f\"\\n--- Feedback (score {record['score']:.1f}) ---\")\n",
    "        for s in record[\"strengths\"]:\n",
    "            print(f\" + {s}\")\n",
    "        for w in record[\"weaknesses\"]:\n",
    "            print(f\" - {w}\")\n",
    "        print(\" Suggested resources:\")\n",
    "        for r in record[\"suggested_resources\"][:5]:\n",
    "            print(f\"   * {r['title']} - {r['url']}\")\n",
    "        print(\"\\n\" + (\"=\"*60) + \"\\n\")\n",
    "\n",
    "\n",
    "    avg_score = sum(a[\"score\"] for a in answers) / len(answers) if answers else 0.0\n",
    "\n",
    "    weak_skills = []\n",
    "    for a in answers:\n",
    "        for res in a[\"suggested_resources\"]:\n",
    "\n",
    "            t = res[\"title\"]\n",
    "            if \":\" in t:\n",
    "                skill_name = t.split(\":\", 1)[0].strip()\n",
    "                weak_skills.append(skill_name)\n",
    "\n",
    "    weak_skills = list(dict.fromkeys(weak_skills))[:6]\n",
    "\n",
    "    roadmap = \"ðŸ“… Suggested Short Roadmap:\\n\"\n",
    "    day = 1\n",
    "    for s in weak_skills:\n",
    "        roadmap += f\"\\nDay {day}-{day+3}: Focus on {s}\\n\"\n",
    "        hits = tavily_search(f\"best free resources to learn {s}\", max_results=4)\n",
    "        for idx, h in enumerate(hits, start=1):\n",
    "            roadmap += f\"  {idx}. {h['title']} ({h['url']})\\n\"\n",
    "        day += 4\n",
    "\n",
    "    session = {\n",
    "        \"role\": role,\n",
    "        \"level\": level,\n",
    "        \"questions\": questions,\n",
    "        \"answers\": answers,\n",
    "        \"avg_score\": avg_score,\n",
    "        \"roadmap\": roadmap,\n",
    "    }\n",
    "    return session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c82e69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Mock Interview CLI (MVP) ===\n",
      "Preparing AI/ML engineer (junior) mock interview with skills: ['machine learning', 'deep learning', 'docker', 'sql']...\n",
      "\n",
      "Q1. (behavioral) Teamwork and Collaboration\n",
      "Describe a time you worked on a project with a team and faced a disagreement. How did you handle it, and what was the outcome?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'grade_answer_with_gemini' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(session[\u001b[33m\"\u001b[39m\u001b[33mroadmap\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[43m_cli\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36m_cli\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m skills = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mYour skills (comma separated): \u001b[39m\u001b[33m\"\u001b[39m).strip()\n\u001b[32m      6\u001b[39m user_skills = [s.strip().lower() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m skills.split(\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m s.strip()]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m session = \u001b[43mrun_text_mock_interview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrole\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_skills\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== SESSION SUMMARY ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAverage score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msession[\u001b[33m'\u001b[39m\u001b[33mavg_score\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mrun_text_mock_interview\u001b[39m\u001b[34m(role, level, user_skills)\u001b[39m\n\u001b[32m     13\u001b[39m user_ans = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mYour answer (type + Enter):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# grade via LLM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m grade = \u001b[43mgrade_answer_with_gemini\u001b[49m(q, user_ans)\n\u001b[32m     16\u001b[39m suggested_skills = grade.get(\u001b[33m\"\u001b[39m\u001b[33msuggested_skills_to_improve\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m     17\u001b[39m suggested_resources = create_suggested_resources(suggested_skills)\n",
      "\u001b[31mNameError\u001b[39m: name 'grade_answer_with_gemini' is not defined"
     ]
    }
   ],
   "source": [
    "def _cli():\n",
    "    print(\"=== Mock Interview CLI (MVP) ===\")\n",
    "    role = input(\"Target role (e.g., Data Scientist): \").strip()\n",
    "    level = input(\"Level (junior/mid/senior): \").strip() or \"junior\"\n",
    "    skills = input(\"Your skills (comma separated): \").strip()\n",
    "    user_skills = [s.strip().lower() for s in skills.split(\",\") if s.strip()]\n",
    "    session = run_text_mock_interview(role, level, user_skills)\n",
    "    print(\"\\n\\n=== SESSION SUMMARY ===\")\n",
    "    print(f\"Average score: {session['avg_score']:.1f}\")\n",
    "    print(session[\"roadmap\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _cli()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e7326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
